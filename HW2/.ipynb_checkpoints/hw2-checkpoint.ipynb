{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 1,
    "id": "kr9vAeEQlRVG"
   },
   "source": [
    "# Домашнее задание 2. Классификация изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 3,
    "id": "BxX49gLclRVJ"
   },
   "source": [
    "В этом задании потребуется обучить классификатор изображений. Будем работать с датасетом, название которого раскрывать не будем. Можете посмотреть самостоятельно на картинки, которые в есть датасете. В нём 200 классов и около 5 тысяч картинок на каждый класс. Классы пронумерованы, как нетрудно догадаться, от 0 до 199. Скачать датасет можно вот [тут](https://yadi.sk/d/BNR41Vu3y0c7qA).\n",
    "\n",
    "Структура датасета простая -- есть директории train/ и val/, в которых лежат обучающие и валидационные данные. В train/ и val/ лежат директориии, соответствующие классам изображений, в которых лежат, собственно, сами изображения.\n",
    " \n",
    "__Задание__. Необходимо выполнить любое из двух заданий\n",
    "\n",
    "1) Добейтесь accuracy **на валидации не менее 0.44**. В этом задании **запрещено** пользоваться предобученными моделями и ресайзом картинок. \n",
    "\n",
    "2) Добейтесь accuracy **на валидации не менее 0.84**. В этом задании делать ресайз и использовать претрейн можно. \n",
    "\n",
    "Напишите краткий отчёт о проделанных экспериментах. Что сработало и что не сработало? Почему вы решили, сделать так, а не иначе? Обязательно указывайте ссылки на чужой код, если вы его используете. Обязательно ссылайтесь на статьи / блогпосты / вопросы на stackoverflow / видосы от ютуберов-машинлернеров / курсы / подсказки от Дяди Васи и прочие дополнительные материалы, если вы их используете. \n",
    "\n",
    "Ваш код обязательно должен проходить все `assert`'ы ниже.\n",
    "\n",
    "Необходимо написать функции `train_one_epoch`, `train` и `predict` по шаблонам ниже (во многом повторяют примеры с семинаров).Обратите особое внимание на функцию `predict`: она должна возвращать список лоссов по всем объектам даталоадера, список предсказанных классов для каждого объекта из даталоалера и список настоящих классов для каждого объекта в даталоадере (и именно в таком порядке).\n",
    "\n",
    "__Использовать внешние данные для обучения строго запрещено в обоих заданиях. Также запрещено обучаться на валидационной выборке__.\n",
    "\n",
    "\n",
    "__Критерии оценки__: Оценка вычисляется по простой формуле: `min(10, 10 * Ваша accuracy / 0.44)` для первого задания и `min(10, 10 * (Ваша accuracy - 0.5) / 0.34)` для второго. Оценка округляется до десятых по арифметическим правилам. Если вы выполнили оба задания, то берется максимум из двух оценок.\n",
    "\n",
    "__Бонус__. Вы получаете 5 бонусных баллов если справляетесь с обоими заданиями на 10 баллов (итого 15 баллов). В противном случае выставляется максимальная из двух оценок и ваш бонус равен нулю.\n",
    "\n",
    "__Советы и указания__:\n",
    " - Наверняка вам потребуется много гуглить о классификации и о том, как заставить её работать. Это нормально, все гуглят. Но не забывайте, что нужно быть готовым за скатанный код отвечать :)\n",
    " - Используйте аугментации. Для этого пользуйтесь модулем `torchvision.transforms` или библиотекой [albumentations](https://github.com/albumentations-team/albumentations)\n",
    " - Можно обучать с нуля или файнтюнить (в зависимости от задания) модели из `torchvision`.\n",
    " - Рекомендуем написать вам сначала класс-датасет (или воспользоваться классом `ImageFolder`), который возвращает картинки и соответствующие им классы, а затем функции для трейна по шаблонам ниже. Однако делать это мы не заставляем. Если вам так неудобно, то можете писать код в удобном стиле. Однако учтите, что чрезмерное изменение нижеперечисленных шаблонов увеличит количество вопросов к вашему коду и повысит вероятность вызова на защиту :)\n",
    " - Валидируйте. Трекайте ошибки как можно раньше, чтобы не тратить время впустую.\n",
    " - Чтобы быстро отладить код, пробуйте обучаться на маленькой части датасета (скажем, 5-10 картинок просто чтобы убедиться что код запускается). Когда вы поняли, что смогли всё отдебажить, переходите обучению по всему датасету\n",
    " - На каждый запуск делайте ровно одно изменение в модели/аугментации/оптимайзере, чтобы понять, что и как влияет на результат.\n",
    " - Фиксируйте random seed.\n",
    " - Начинайте с простых моделей и постепенно переходите к сложным. Обучение лёгких моделей экономит много времени.\n",
    " - Ставьте расписание на learning rate. Уменьшайте его, когда лосс на валидации перестаёт убывать.\n",
    " - Советуем использовать GPU. Если у вас его нет, используйте google colab. Если вам неудобно его использовать на постоянной основе, напишите и отладьте весь код локально на CPU, а затем запустите уже написанный ноутбук в колабе. Авторское решение задания достигает требуемой точности в колабе за 15 минут обучения.\n",
    " \n",
    "Good luck & have fun! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": 4,
    "id": "LKcSNj4tlRVK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "# You may add any imports you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np # for transformation\n",
    "\n",
    "import torch # PyTorch package\n",
    "import torchvision # load datasets\n",
    "import torchvision.transforms as transforms # transform data\n",
    "import torch.nn as nn # basic building block for neural neteorks\n",
    "import torch.nn.functional as F # import convolution functions like Relu\n",
    "import torch.optim as optim # optimzer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RytEDW0ylRVN"
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uSqSsFg4lRVN"
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform):\n",
    "        # YOUR CODE\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # YOUR CODE\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # YOUR CODE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "cell_id": 5,
    "id": "QEdDQtHdlRVO"
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([ \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    #transforms.Resize((5,5)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "val_transform = transforms.Compose([ \n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    #transforms.Resize((5,5)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# YOU CAN DEFINE AUGMENTATIONS HERE\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder('/Users/rinokus/Documents/Projects/HSE-DL-HW/HW2/data/train', transform=train_transform)\n",
    "val_dataset = torchvision.datasets.ImageFolder('/Users/rinokus/Documents/Projects/HSE-DL-HW/HW2/data/val', transform=val_transform)\n",
    "# REPLACE ./dataset/dataset WITH THE FOLDER WHERE YOU DOWNLOADED AND UNZIPPED THE DATASET\n",
    "# OR USE torchvision.datasets.ImageFolder INSTEAD OF MyDataset\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_width(width, kernel, padding, stride):\n",
    "    return int((width + 2 * padding - kernel - 1) / stride + 1)\n",
    "\n",
    "#classes = labels.columns[1:]\n",
    "\n",
    "in_width = 64\n",
    "\n",
    "kernel = 5\n",
    "pool_kernel = 2\n",
    "padding = int(kernel/2)\n",
    "stride = 1\n",
    "\n",
    "c1_in = 3\n",
    "c1_kernel = 9\n",
    "c1_out = 64\n",
    "c1_conv_width = get_output_width(in_width, c1_kernel, int(c1_kernel/2), stride)\n",
    "c1_pooled_width = get_output_width(c1_conv_width, int(pool_kernel/2), 0, pool_kernel)\n",
    "\n",
    "c2_kernel = 5\n",
    "c2_out = 96\n",
    "c2_conv_width = get_output_width(c1_pooled_width, c2_kernel, int(c2_kernel/2), stride)\n",
    "c2_pooled_width = get_output_width(c2_conv_width, pool_kernel, int(pool_kernel/2), pool_kernel)\n",
    "\n",
    "full_1_in = c2_out * c2_pooled_width * c2_pooled_width\n",
    "full_1_out = int(full_1_in / 8)\n",
    "full_2_out = int(full_1_out/4)\n",
    "full_3_out = 200\n",
    "\n",
    "class CringeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CringeNet, self).__init__()\n",
    "        '''\n",
    "        These are the layers in the network, and their attributes are the\n",
    "        weights and biases of the neurons.\n",
    "        '''\n",
    "        self.conv1 = nn.Conv2d(in_channels=c1_in,\n",
    "                               out_channels=c1_out,\n",
    "                               kernel_size=c1_kernel,\n",
    "                               stride=stride,\n",
    "                               padding=padding,\n",
    "                               padding_mode='zeros')\n",
    "        self.pool = nn.MaxPool2d(pool_kernel)\n",
    "        self.conv2 = nn.Conv2d(in_channels=c1_out,\n",
    "                               out_channels=c2_out,\n",
    "                               kernel_size=c2_kernel,\n",
    "                               stride=stride,\n",
    "                               padding=padding,\n",
    "                               padding_mode='zeros')\n",
    "        self.conv2_bn = nn.BatchNorm2d(c2_out)\n",
    "        self.fc1 = nn.Linear(full_1_in, full_1_out)\n",
    "        self.fc2 = nn.Linear(full_1_out, full_2_out)\n",
    "        self.fc3 = nn.Linear(full_2_out, full_3_out)\n",
    "        self.dropout = nn.Dropout(p=0.15)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2_bn(self.conv2(x))))\n",
    "        x = x.view(x.size()[0],-1)\n",
    "        x = F.relu(self.dropout(self.fc1(x)))\n",
    "        x = F.relu(self.dropout(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "cell_id": 6,
    "id": "mrg4Yj0VlRVP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests passed\n"
     ]
    }
   ],
   "source": [
    "# Just very simple sanity checks\n",
    "assert isinstance(train_dataset[0], tuple)\n",
    "assert len(train_dataset[0]) == 2\n",
    "assert isinstance(train_dataset[1][1], int)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RlSlmyjlRVP"
   },
   "source": [
    "### Вспомогательные функции, реализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "cell_id": 7,
    "id": "yYG2-Cq8lRVQ"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataloader, criterion, optimizer, device=\"cuda:0\"):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accur = 0.0\n",
    "    with tqdm(total=len(train_dataloader), file=sys.stdout) as prbar:\n",
    "        for inputs, labels in train_dataloader:\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            prbar.set_description(\n",
    "                f\"Loss: {round(loss.item(), 4)}\"\n",
    "                f\"Accur: {torch.sum(preds == labels.data)}\"\n",
    "            )\n",
    "            prbar.update(1)\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running_accur += torch.sum(preds == labels.data)\n",
    "    dataset_len = len(train_dataloader.dataset)\n",
    "    return running_loss / dataset_len, running_accur / dataset_len\n",
    "\n",
    "\n",
    "def predict(model, val_dataloder, criterion, device=\"cuda:0\"):\n",
    "    model.eval()\n",
    "    # YOUR CODE\n",
    "    # PREDICT FOR EVERY ELEMENT OF THE VAL DATALOADER AND RETURN CORRESPONDING LISTS\n",
    "    return losses, predicted_classes, true_classes\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, criterion, optimizer, device=\"cuda:0\", n_epochs=10, scheduler=None):\n",
    "    model.to(device)\n",
    "    last_loss = 0.0\n",
    "    last_accur = 0.0\n",
    "    for epoch in range(n_epochs):\n",
    "        last_loss, last_accur = train_one_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxR3gfcilRVW"
   },
   "source": [
    "### Обучение модели, запуски экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "cell_id": 8,
    "id": "JXFJ6oS8lRVX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CringeNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#scheduler = # LR SCHEDULE THAT YOU PROBABLY CHOOSE\n",
    "n_epochs = 10\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 9,
    "id": "CesoOl6BlRVY"
   },
   "source": [
    "Простой тест на проверку правильности написанного кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "cell_id": 10,
    "id": "B_LB2jn6lRVY"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-7ba8bf5b2b50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tests passed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
    "assert len(predicted_labels) == len(val_dataset)\n",
    "accuracy = accuracy_score(predicted_labels, true_labels)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 11,
    "id": "tS-LLiXUlRVY"
   },
   "source": [
    "Запустить обучение можно в ячейке ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "cell_id": 12,
    "id": "ECIzZ_RYlRVZ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677a065f42924813850eb7d3850dfb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1563), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/rinokus/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/rinokus/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/rinokus/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/rinokus/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-c9060e7e8685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-96-6ad75f1f44a9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, criterion, optimizer, device, n_epochs, scheduler)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mlast_accur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mlast_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_accur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-6ad75f1f44a9>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, criterion, optimizer, device, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImVW8_EXlRVZ"
   },
   "source": [
    "### Проверка полученной accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 13,
    "id": "FmR-elhJlRVZ"
   },
   "source": [
    "После всех экспериментов которые вы проделали, выберите лучшую из своих моделей, реализуйте и запустите функцию `evaluate`. Эта функция должна брать на вход модель и даталоадер с валидационными данными и возврашать accuracy, посчитанную на этом датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "id": "3TGH0EFalRVb"
   },
   "outputs": [],
   "source": [
    "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
    "assert len(predicted_labels) == len(val_dataset)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Оценка за это задание составит {} баллов\".format(min(5, 5 * accuracy / 0.44)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ \n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.ImageFolder('/Users/rinokus/Downloads/dataset/dataset/train', transform=transform)\n",
    "val_data = torchvision.datasets.ImageFolder('/Users/rinokus/Downloads/dataset/dataset/val', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "#test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=3, use_auxiliary=True, num_classes=200):\n",
    "        super(Inception, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.conv2 = ConvBlock(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        self.use_auxiliary = use_auxiliary\n",
    "        if use_auxiliary:\n",
    "            self.auxiliary4a = Auxiliary(512, num_classes)\n",
    "            self.auxiliary4d = Auxiliary(528, num_classes)\n",
    "        \n",
    "        self.inception3a = InceptionBlock(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.inception4a = InceptionBlock(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = InceptionBlock(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = InceptionBlock(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = InceptionBlock(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5a = InceptionBlock(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = InceptionBlock(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = None\n",
    "        z = None\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception4a(x)\n",
    "        if self.training and self.use_auxiliary:\n",
    "            y = self.auxiliary4a(x)\n",
    "        \n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        if self.training and self.use_auxiliary:\n",
    "            z = self.auxiliary4d(x)\n",
    "        \n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, im_channels, num_1x1, num_3x3_red, num_3x3, num_5x5_red, num_5x5, num_pool_proj):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        \n",
    "        self.one_by_one = ConvBlock(im_channels, num_1x1, kernel_size=1)\n",
    "        \n",
    "        self.tree_by_three_red = ConvBlock(im_channels, num_3x3_red, kernel_size=1)  \n",
    "        self.tree_by_three = ConvBlock(num_3x3_red, num_3x3, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.five_by_five_red = ConvBlock(im_channels, num_5x5_red, kernel_size=1)\n",
    "        self.five_by_five = ConvBlock(num_5x5_red, num_5x5, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.pool_proj = ConvBlock(im_channels, num_pool_proj, kernel_size=1)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x1 = self.one_by_one(x)\n",
    "        \n",
    "        x2 = self.tree_by_three_red(x)\n",
    "        x2 = self.tree_by_three(x2)\n",
    "        \n",
    "        x3 = self.five_by_five_red(x)\n",
    "        x3 = self.five_by_five(x3)\n",
    "        \n",
    "        x4 = self.maxpool(x)\n",
    "        x4 = self.pool_proj(x4)\n",
    "        \n",
    "        x = torch.cat([x1, x2, x3, x4], 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Auxiliary(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Auxiliary, self).__init__()\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
    "        self.conv1x1 = ConvBlock(in_channels, 128, kernel_size=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Inception()\n",
    "\n",
    "#define the device to use\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move the model to the device\n",
    "model.to(device)\n",
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define everything we need for training\n",
    "epochs = 3 #50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=50, use_auxiliary=True):\n",
    "    \n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        for phase in ['train', 'val']: # Each epoch has a training and validation phase\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                print(\"Train phase\")\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                print(\"Val phase\")\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            with tqdm(total=len(dataloaders[phase]), file=sys.stdout) as prbar:\n",
    "                for inputs, labels in dataloaders[phase]: # Iterate over data\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    optimizer.zero_grad() # Zero the parameter gradients\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'train'): # Forward. Track history if only in train\n",
    "                        if phase == 'train': # Backward + optimize only if in training phase\n",
    "                            if use_auxiliary:\n",
    "                                outputs, aux1, aux2 = model(inputs)\n",
    "                                loss = criterion(outputs, labels) + 0.3 * criterion(aux1, labels) + 0.3 * criterion(aux2, labels)\n",
    "                            else:\n",
    "                                outputs, _, _ = model(inputs)\n",
    "                                loss = criterion(outputs, labels)\n",
    "\n",
    "                            _, preds = torch.max(outputs, 1)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                        if phase == 'val':\n",
    "                            outputs, _, _ = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # Statistics\n",
    "                    prbar.set_description(\n",
    "                        f\"Loss: {round(loss.item(), 4)}\"\n",
    "                    )\n",
    "                    prbar.update(1)\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "\n",
    "            if phase == 'val': # Adjust learning rate based on val loss\n",
    "                lr_scheduler.step(epoch_loss)\n",
    "\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _ = train_model(model, {\"train\": train_loader, \"val\": val_loader}, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 15,
    "id": "pT8vfPSolRVb"
   },
   "source": [
    "### Отчёт об экспериментах \n",
    "\n",
    "текст писать тут"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw01_part2_tony_upd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "max_cell_id": 35
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
